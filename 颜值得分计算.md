# 颜值得分计算
## 项目参考
> 1. Face Rank - Rank Face by CNN Model based on TensorFlow（*Keras Version*）https://github.com/fendouai/FaceRank/tree/master/FaceRank_with_keras
> 2. A deep learning based model to judge the AQ, Appearance Quotient, of faces. (*For Chinese Young Girls Only*) https://github.com/Entropy-xcy/RankFace

## 库版本
**About python**

- Python: 3.6.9 :: *Anaconda, Inc*
- Tensorflow: 1.13.1
- Keras: 2.3.1
- tensorflowjs: 1.2.10.1

**About JS**

- tfjs-node@1.3.2

## 模型介绍
### 原始项目模型
**Model Summary**
```python
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 128, 128, 32)      896       
_________________________________________________________________
activation_1 (Activation)    (None, 128, 128, 32)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 126, 126, 32)      9248      
_________________________________________________________________
activation_2 (Activation)    (None, 126, 126, 32)      0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 63, 63, 32)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 63, 63, 32)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 127008)            0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16257152  
_________________________________________________________________
activation_3 (Activation)    (None, 128)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 16,267,425
Trainable params: 16,267,425
Non-trainable params: 0
_________________________________________________________________
```
该模型的最后一层使用了单元数为1的全连接层，这表示了该模型最终的输出可能是以小数呈现的颜值分数。

### 模型损失函数
**训练代码**

```python
def main():
    train_x, train_y = load_image_data('./data/')
    model = make_network()

    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])
    hist = model.fit(train_x, train_y, batch_size=100, epochs=100, verbose=1)

    model.evaluate(train_x, train_y)
    model.save('faceRank.h5')
```
从 loss='mean_squared_error'看出该模型训练时采用的是均方误差（*MSE*）

![](https://img-blog.csdn.net/20180426171112523?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpdXlpbmd5aW5nMDQxOA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

模型训练（*fit*）时使用的 **Y（*train_y*）**是一个表示颜值的数值，而模型的输出**y**之前分析过也是一个数值，则损失的计算公式简化为 $ loss = \sqrt{Y^{2}-y^{2}}$，模型的优化目标是减小loss，即 $y \to Y$ 也就是使模型的训练集输出尽可能接近训练的标签颜值分。 

### 更换模型损失
因为需要的颜值结果为0~100之间的整数，所以可以认为将人脸的颜值分成101类，经过模型映射成哪一类，那么颜值就是相应的数值，因此可以将原问题转化为101类的多分类问题。为了进行多分类的模型训练，需要调整模型结构和更换损失函数。

**模型构造**
```python
def make_network():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), padding='same', input_shape=(128, 128, 3)))
    model.add(Activation('relu'))
    model.add(Conv2D(32, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.5))

    model.add(Flatten())
    model.add(Dense(128))
    model.add(Activation('relu'))
    model.add(Dropout(0.5))
    model.add(Dense(101))
    model.add(Activation('softmax'))
```

**Model Summary**
```python
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 128, 128, 32)      896       
_________________________________________________________________
activation_1 (Activation)    (None, 128, 128, 32)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 126, 126, 32)      9248      
_________________________________________________________________
activation_2 (Activation)    (None, 126, 126, 32)      0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 63, 63, 32)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 63, 63, 32)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 127008)            0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16257152  
_________________________________________________________________
activation_3 (Activation)    (None, 128)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 101)               13029     
_________________________________________________________________
activation_4 (Activation)    (None, 101)               0         
=================================================================
Total params: 16,280,325
Trainable params: 16,280,325
Non-trainable params: 0
_________________________________________________________________
```
与原本的模型结构不同的是，将最后的全连接层的单元数改成101个，使结果能够输出一个长度为101的一维数组，并且加上了softmax的激活函数层。

**训练代码**
```python
if __name__ == '__main__':
    root = './data/ImagesCut/Images/'
    valdir = './data/train_test_files/All_labels.txt'
    train_x, train_y = read_img(root, valdir)
    train_y = np_utils.to_categorical(train_y, num_classes=101)
    model = make_network()
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    hist = model.fit(train_x, train_y, validation_split=0.2, batch_size=32, epochs=15, initial_epoch=12, verbose=1)
    model.save("E:/tensorflow/FaceRank-master/FaceRank_with_keras/model/faceRankResNet50-c101-sgd-ep15.h5")
```
模型采用的损失函数是 **categorical_crossentropy**（*交叉熵损失函数*）。**categorical_crossentropy**和**softmax**的组合往往是多分类模型的常用方法。
这里稍微介绍一下为什么要用softmax激活函数和交叉熵损失函数
**softmax处理流程**

![微信截图_20181022153759.png-66.5kB](https://segmentfault.com/img/remote/1460000017321270?w=809&h=408)

![image_1cqdbm3l51ntme1g1i4pnd1u2t4c.png-186.3kB](https://segmentfault.com/img/remote/1460000017321271?w=678&h=395)

softmax在每个节点的激励函数都是 $\sigma_i(z) =  \frac {e^{z_j}}{\sum_{j=1}^me^{z_j}}$。与简单的激励函数relu等不同的是，softmax的结果不仅仅与输入的节点有关，还受全部的节点影响。softmax的结果简单来看可以相当于得到该输出节点的占全部输出的比例，如果用分类来理解就是转化成概率分布。

转化成概率分布之后，接下来交叉熵来获得训练时的损失。交叉熵刻画的是实际输出概率和期望输出概率的距离，交叉熵的值越小，则两个概率分布越接近，即实际与期望差距越小。假设概率分布为$p$期望输出，概率$q$分布为实际输出，$H(x)$为交叉熵。则:

$H(x) = - \sum_{i=1}^Kp(x_i)log(q(x_i))$ 

因此交叉熵损失函数的定义为:

$J = - \sum_{i=1}^Ky_ilog(p_i) $

其中:
- K : 类别的数量
- y : 是否是类别$c, y\in(0, 1)$
- p : 样本属于类别$c$的概率

在使用交叉熵损失函数的时候通常把期望分布用one-hot编码（*独热编码*）来表示，这样的话给定的$y_i$中只会有一个类别是1，其他类别都是0，所以BP更新权重的时候，梯度可化为

$ \frac {∂J_i}{∂\omega_n} =  \frac {∂y_i}{∂\omega_n} = (p(i) - 1)\omega_n$

这样就很方便的计算出梯度了。

### 使用预训练模型
在观察刚刚修改后的模型的时候发现，
```python
dense_1 (Dense)              (None, 128)               16257152  
_________________________________________________________________
activation_3 (Activation)    (None, 128)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 101)               13029     
```
该模型的dense_1的output仅为128而dense_2的output却达到了101。这表明了模型深度不够，使用101单元的dense有些不妥。所以这里我使用了迁移学习的方法，利用预训练的resnet50来构建项目模型。
**构建模型代码**
```python
def build_model():
    base_model = ResNet50(include_top=False, weights='imagenet',
             input_tensor=None, input_shape=(224,224,3),
             pooling='avg')
    image_input = base_model.input
    x = base_model.layers[-1].output
    x = Dense(101)(x)
    out = Activation('softmax')(x)
    faceRankModel = Model(image_input, out)
    # for i in  range(0, len(faceRankModel.layers)-2):
    #     faceRankModel.layers[i].trainable = False
    faceRankModel.summary()
    return faceRankModel
```
<font color=red face="黑体"> *Keras可以通过applications接口下载并导入一部分常见模型的预训练模型。</font>

## 数据处理
### 训练数据集

> **SCUT-FBP5500**  A diverse benchmark database (Size = 172MB) for multi-paradigm facial beauty prediction is now released by Human Computer Intelligent Interaction Lab of South China University of Technology. https://github.com/HCIILAB/SCUT-FBP5500-Database-Release/tree/master

SCUT-FBP5500 数据集发布于 2017 年，包含 5500 个正面人脸，年龄分布位 15 岁至 60 岁。人脸照片包括 2000 亚洲女性，2000 亚洲男性，750 高加索男性和 750 高加索女性。每张图由 60 人进行评分，共 5 个等级。

### 训练数据处理
1. **裁剪并更改图片形状**
尽管数据集已经经过处理，但图片的四围还是有白边。并且在js上不论通过mtcnn还是faceapi来delectface，都与训练数据集都有所不同，这会使模型验证的时候出现误差，所以需要先对图片进行裁剪。在原项目中使用的是face_recognition库来检测人脸框并以此截取图片，但在本项目中使用的是facenet的align_dataset_mtcnn.py，二者的结果并无较大差异。

```cmd
$ E:\tensorflow\FaceRank-master\FaceRank_with_keras\data\image E:\tensorflow\FaceRank-master\FaceRank_with_keras\data\ImagesCut --image_size 224 --margin 0 --random_order
```
这里我直接将图片大小resize成和输入形状一样，省去了载入数据集时候的resize。

**部分结果**

| 原数据集图片  | 处理后的图片 |
| ------------- | ------------- |
|![](https://github.com/zhengh12/data/blob/master/AF818.jpg?raw=true) |![](https://github.com/zhengh12/data/blob/master/AF818.png?raw=true) |
|![](https://github.com/zhengh12/data/blob/master/AM823.jpg?raw=true) |![](https://github.com/zhengh12/data/blob/master/AM823.png?raw=true) |
|![](https://github.com/zhengh12/data/blob/master/CM439.jpg?raw=true) |![](https://github.com/zhengh12/data/blob/master/CM439.png?raw=true)|

2. **数据处理**
为了使训练更快收敛，往往都需要对输入的图片数据进行处理。下面是数据处理的代码:

```python
def read_img(root, filedir):
    # Data loading
    with open(filedir, 'r') as f:
        lines = f.readlines()
    image_data_list = []
    label = []
    for i in tqdm(range(len(lines))):
        linesplit = lines[i].split('\n')[0].split(' ')
        addr = linesplit[0]
        pth = addr.split('.')
        path = pth[0] + '.png'
        target = linesplit[1]
        url = os.path.join(root, path)
        # print(url, os.path.isfile(url))
        if os.path.isfile(url):
            image = load_img(url, target_size=(224, 224))
            image_data_list.append(img_to_array(image))
            label.append(int(float(target)*20+0.5))
    img_data = np.array(image_data_list)
    img_data = img_data.astype('float32')
    img_data /= 255
    img_data -= np.array([0.485, 0.456, 0.406])
    img_data /= np.array([0.229, 0.224, 0.225])
    print("lodingImgOver", len(img_data))
    return img_data, label
```

对输入图片数据(*X*)归一化($\frac {X_i - X_{min}}{X_{max} - X_{min}}$)使取值范围介于0~1，再对输入数据X标准化($\frac {X - \mu}{\sigma}$)，这里使用的$\mu$和$\sigma$使用的是pytorch上通用的值。

对输入标签数据(*Y*)将其通过
```python
int(float(target)*20+0.5) 
```
转化成取值范围为0-100的整数。再通过
```python
train_y = np_utils.to_categorical(train_y, num_classes=101)
```
将int数组变为one-hot数组。至此完成训练数据的准备。

## 训练步骤
### step1 仅训练新增层
为了加快训练速度，在生成模型的时候先设置resnet50原有的层不进行训练，仅训练新增层。
```python
for i in  range(0, len(faceRankModel.layers)-2):
     faceRankModel.layers[i].trainable = False
```
以此来判断模型在训练集上是否能快速收敛。
```python
 root = './data/ImagesCut/Images/'
    valdir = './data/train_test_files/All_labels.txt'
    train_x, train_y = read_img(root, valdir)
    train_y = np_utils.to_categorical(train_y, num_classes=101)
    model = build_model()
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    hist = model.fit(train_x, train_y, validation_split=0.2, batch_size=32, epochs=45，rbose=1)
```
优化器使用adam，训练集和验证集的划分使用留出法，通过train_test_split参数将训练集和验证集划分成1:4，并未使用交叉验证。训练45轮次之后loss为0.3890，acc为0.9328。
### step2 微调整个模型
将模型的所有层设置成可训练。
```python
root = './data/ImagesCut/Images/'
    valdir = './data/train_test_files/All_labels.txt'
    train_x, train_y = read_img(root, valdir)
    train_y = np_utils.to_categorical(train_y, num_classes=101)
    model = load_model("./model/faceRankResNet50-c101-ep45-loss0.3890-acc0.9328.h5")
    for i in  range(0, len(model.layers)):
        model.layers[i].trainable = True
    sgd = SGD(lr=0.001, momentum=0, decay=1e-6, nesterov=False)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    hist = model.fit(train_x, train_y, validation_split=0.2, batch_size=32, epochs=15, initial_epoch=0, verbose=1)
    model.save("E:/tensorflow/FaceRank-master/FaceRank_with_keras/model/faceRankResNet50-c101-sgd-ep15.h5")
```
第二次训练使用sgd优化器，并且设置一个较小的学习率。因为要微调整个网络，如果使用较大的学习率会使网络权重更新的较大反而使得loss升高，从较高的地方开始收敛，这样的话之前的训练都白费了。训练15轮之后loss下降到0.0306，val_loss为0.2966， val_acc为0.9591，训练结束后将模型保存为h5文件。

## 模型验证
**验证代码**
```python
model = load_model("./model/faceRankResNet50-c101-sgd-ep15-loss0.0306-val_loss0.2966-val_acc0.9591.h5")
image = load_image('./data/ImagesCut/Images/AF1273.png')
res11 = model.predict(image)[0]
res1 = 0
for i in range(0,101):
    res1 = res1 + res11[i]*i
print(res1)
print(res11)
```

**模型结果**

```python
AF818 faceRank 61.13375775076566
AM823 faceRank 53.0075929210527
CM439 faceRank 64.90084276288023
```

**实际值**
AF818.jpg  3.05  61
AM823.jpg  2.666667  53
CM439.jpg  3.25  65


## 模型转化
通过tensorflowjs的接口将Keras模型转化成tensorflow.js模型
```python
model = load_model("./model/faceRankResNet50-c101-sgd-ep15-loss0.0306-val_loss0.2966-val_acc0.9591.h5")
tfjs.converters.save_keras_model(model, "./tfjsmodel2")
```

## Node上运行模型

```javascript
async function CalFaceRank(filePath){
    const [Pnet, Rnet, Onet] = await detectFace.loadModel(config.modelPath.pModelPath, config.modelPath.rModelPath, config.modelPath.oModelPath)
    let img = fs.readFileSync(filePath)
    let imgarr = tf.node.decodeImage(img)
    let rectangles = detectFace.detectFace(imgarr, config.mtcnnParam.threshold, Pnet, Rnet, Onet)
    // console.log(rectangles)
    if(rectangles.length == 0){
        return "error"
    }
    imgTensorFace = imgarr.slice([rectangles[0][1], rectangles[0][0]], [rectangles[0][3]-rectangles[0][1], rectangles[0][2]-rectangles[0][0]])
    imgTensorFace = tf.div(imgTensorFace, tf.scalar(255))
    imgTensorFace = tf.div(tf.sub(imgTensorFace, tf.tensor([0.485, 0.456, 0.406])), tf.tensor([0.229, 0.224, 0.225]))
    // imgTensorFace = tf.image.resizeNearestNeighbor(imgTensorFace, [224,224])
    imgTensorFace = tf.image.resizeBilinear(imgTensorFace, [224,224])
    imgTensorFace = imgTensorFace.expandDims(0)
    const modelPath = "./public/model/faceRankResNet50-c101-sgd-ep15-loss0.0306-val_loss0.2966-val_acc0.9591/model.json"
    const model = await tf.loadLayersModel('file://'+modelPath);
    // model.summary()
    const res = model.predict(imgTensorFace)
    let resArr = res.arraySync()[0]
    let max = 0
    for(let i=0; i<resArr.length; i++){
        max = resArr[i] > max ? resArr[i] : max
    }
    let Rank = resArr.indexOf(max)
    let fn = filePath.split("/")
    console.log(fn[fn.length-1], "人脸得分", Rank)
    return Rank
}
```

```javascript
	let img = fs.readFileSync(filePath)
    let imgarr = tf.node.decodeImage(img)
```
这一段代码是读取指定路径的图片数据，并将其转化为tensor型数据。

```javascript
	const [Pnet, Rnet, Onet] = await detectFace.loadModel(config.modelPath.pModelPath, config.modelPath.rModelPath, config.modelPath.oModelPath)
	let rectangles = detectFace.detectFace(imgarr, config.mtcnnParam.threshold, Pnet, Rnet, Onet)
    // console.log(rectangles)
    if(rectangles.length == 0){
        return "error"
    }
    imgTensorFace = imgarr.slice([rectangles[0][1], rectangles[0][0]], [rectangles[0][3]-rectangles[0][1], rectangles[0][2]-rectangles[0][0]])
```
这段代码是利用之前实现的detectFace接口检测人脸框并截取出人脸数据。

```javascript
	imgTensorFace = tf.div(imgTensorFace, tf.scalar(255))
    imgTensorFace = tf.div(tf.sub(imgTensorFace, tf.tensor([0.485, 0.456, 0.406])), tf.tensor([0.229, 0.224, 0.225]))
    // imgTensorFace = tf.image.resizeNearestNeighbor(imgTensorFace, [224,224])
    imgTensorFace = tf.image.resizeBilinear(imgTensorFace, [224,224])
    imgTensorFace = imgTensorFace.expandDims(0)
```
这段代码是仿照训练数据的处理流程进行处理包括归一化和标准化，并将人脸数据tensor，resize成与模型输入相同的形状，这样才可以得到与测试集相同的结果。

```javascript
	const modelPath = "./public/model/faceRankResNet50-c101-sgd-ep15-loss0.0306-val_loss0.2966-val_acc0.9591/model.json"
    const model = await tf.loadLayersModel('file://'+modelPath);
    // model.summary()
    const res = model.predict(imgTensorFace)
    let resArr = res.arraySync()[0]
    let max = 0
    for(let i=0; i<resArr.length; i++){
        max = resArr[i] > max ? resArr[i] : max
    }
    let Rank = resArr.indexOf(max)
    let fn = filePath.split("/")
    console.log(fn[fn.length-1], "人脸得分", Rank)
```
这段代码

```javascript
[ 1.014011999367348e-10, 1.0458051785677824e-10, 2.2019504275494484e-10, 6.566287791276437e-11, 5.25316526389652e-11, 7.122500506051566e-11, 9.554110280696193e-11, 1.410514760546988e-10, 6.905576804827618e-11, 6.049701017918352e-11, 6.049816897446547e-11, 7.53196255365296e-11, 4.1010913415240324e-11, 3.9441915011817485e-11, 9.27720400500931e-11, 2.3479810110083044e-11, 7.037990329417099e-11, 6.306623973051373e-11, 2.501741591665052e-10, 3.1701176406162546e-11, 5.863115304675579e-14, 2.7414247087687027e-12, 8.714895346811657e-18, 9.26046484503798e-14, 5.256512586315765e-11, 3.4629712156652656e-14, 5.296118361572399e-13, 3.0402275748991903e-12, 2.1498704170521375e-15,9.439581820196163e-14, 5.905694549791221e-15, 3.7588803597499165e-12,1.5880483709182318e-14, 8.462365465485011e-13, 1.5107481488030317e-8, 3.756056663611895e-12, 7.397450985102694e-10, 8.076833424297547e-9, 7.949828528808212e-8,
7.728320952082868e-7, 3.9951706298779754e-7, 6.745796365947854e-9, 0.0000320297185680829,0.00010744897008407861,0.0000016183266779989935,
3.1225596330841654e-7, 9.869864925349248e-7, 0.000011192777492396999,
0.000029805592930642888,0.00005161578155821189,0.00005434250851976685,
0.0000368574692402035, 0.00012220234202686697, 0.0002407676074653864,
0.0001269879430765286, 0.0001295990514336154, 0.00001557167706778273,
  0.00004105463085579686,
  0.000019390023226151243,
  0.000050810900575015694,
  0.0000031787960779183777,
  0.9248117804527283,
  0.0000033234418879146688,
  0.00019934387819375843,
  0.000016131469237734564,
  0.000002899304945458425,
  0.000002249748604299384,
  0.0002276239829370752,
  0.0003293496265541762,
  0.0000061102155086700805,
  0.0007821087492629886,
  0.00009252703603124246,
  0.008965694345533848,
  0.00004908107075607404,
  0.012921907007694244,
  0.013425871729850769,
  0.010387957096099854,
  0.0008531476487405598,
  0.0002552892838139087,
  0.011305350810289383,
  0.007112632971256971,
  0.00006939081504242495,
  0.00023520099057350308,
  0.001965111354365945,
  0.0006194853922352195,
  0.000054395604820456356,
  0.00008329311822308227,
  0.003973368555307388,
  0.000018915148757514544,
  0.00014554269728250802,
  7.170659728217288e-7,
  0.000007979247129696887,
  1.2922883249610884e-11,
  2.8016280850806652e-8,
  1.4592967459425843e-11,
  8.069307375069279e-12,
  7.515574274030712e-11,
  1.2867205911870627e-10,
  3.2225833113130875e-11,
  6.787813366937456e-11,... 1 more item ]
```